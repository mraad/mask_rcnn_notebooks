{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getenv(\"MRCNN_HOME\", \"/Mask_RCNN\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "    \n",
    "DATA_PATH = \"/data/RCNNTanks256Train/Yanbu\"\n",
    "\n",
    "IMG_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tf backend to allow memory to grow, instead of claiming everything\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "\n",
    "def get_session():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    return tf.Session(config=config)\n",
    "\n",
    "# set the modified tf session as backend in keras\n",
    "keras.backend.tensorflow_backend.set_session(get_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRConfig(Config):\n",
    "    \"\"\"Configuration for training on the Miami buildings dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"tank\"\n",
    "\n",
    "    BATCH_SIZE = 8\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  \n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = IMG_SIZE\n",
    "    IMAGE_MAX_DIM = IMG_SIZE\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  \n",
    "    # RPN_ANCHOR_SCALES = (10, 20, 40, 80, 160)  \n",
    "    \n",
    "    # Ratios of anchors at each cell (width/height)\n",
    "    # A value of 1 represents a square anchor, and 0.5 is a wide anchor\n",
    "#     RPN_ANCHOR_RATIOS = [0.25, 1, 4]\n",
    "    \n",
    "    \n",
    "    # Loss weights for more precise optimization.\n",
    "    # Can be used for R-CNN training setup.\n",
    "#     LOSS_WEIGHTS = {\n",
    "#         \"rpn_class_loss\": 1.,\n",
    "#         \"rpn_bbox_loss\": 1.,\n",
    "#         \"mrcnn_class_loss\": 1.,\n",
    "#         \"mrcnn_bbox_loss\": 1.,\n",
    "#         \"mrcnn_mask_loss\": 1.1\n",
    "#     }    \n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 256\n",
    "    \n",
    "#     ROI_POSITIVE_RATIO = 0.5 #makes no positive effect\n",
    "\n",
    "    # Max number of final detections\n",
    "    DETECTION_MAX_INSTANCES = 100\n",
    "\n",
    "    # Maximum number of ground truth instances to use in one image\n",
    "    MAX_GT_INSTANCES = 100\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 1\n",
    "    \n",
    "    # Image mean (RGB)\n",
    "    MEAN_PIXEL = np.array([131.84381436753546, 125.43039054432134, 113.32320930217874])\n",
    "    LEARNING_RATE = 1.e-4\n",
    "    WEIGHT_DECAY = 1.e-5\n",
    "\n",
    "\n",
    "config = MRConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import skimage\n",
    "\n",
    "class MRDataset(utils.Dataset):\n",
    "\n",
    "    def load(self, dataset_dir):\n",
    "        # Add classes\n",
    "        self.add_class(\"tank\", 1, \"Tank\")\n",
    "        \n",
    "        #loading images\n",
    "        self._image_dir = os.path.join(dataset_dir, \"images/\")\n",
    "        self._label_dir = os.path.join(dataset_dir, \"labels/\")\n",
    "        for i, f in enumerate(glob.glob(os.path.join(self._image_dir, \"*.tif\"))):\n",
    "            _, filename = os.path.split(f)\n",
    "            self.add_image(\"tank\",\n",
    "                           image_id=i,\n",
    "                           path=f,\n",
    "                           width=config.IMAGE_MAX_DIM,\n",
    "                           height=config.IMAGE_MAX_DIM,\n",
    "                           filename=filename)\n",
    "            \n",
    "    def load_mask(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        fname = info[\"filename\"]\n",
    "        masks = []\n",
    "        class_ids = []\n",
    "        #looping through all the classes, loading and processing corresponding masks\n",
    "        for ci in self.class_info:\n",
    "            class_name = ci[\"name\"]\n",
    "            class_id = ci[\"id\"]\n",
    "            try:\n",
    "                m_src = skimage.io.imread(os.path.join(self._label_dir, class_name, fname))\n",
    "            except:\n",
    "                #no file with masks of this class found\n",
    "                continue                \n",
    "            #making individual masks for each instance\n",
    "            instance_ids = np.unique(m_src)\n",
    "            for i in instance_ids:\n",
    "                if i > 0:\n",
    "                    m = np.zeros(m_src.shape)\n",
    "                    m[m_src==i] = i\n",
    "                    if np.any(m==i):\n",
    "                        masks.append(m)\n",
    "                        class_ids.append(class_id)\n",
    "        if len(masks) == 0:\n",
    "            masks.append(np.zeros(m_src.shape))\n",
    "            class_ids.append(1)            \n",
    "        masks = np.stack(masks, axis=-1)                    \n",
    "        return masks.astype(np.bool), np.array(class_ids, dtype=np.int32)\n",
    "        \n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"tank\":\n",
    "            return info[\"path\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = MRDataset()\n",
    "dataset_train.load(os.path.join(DATA_PATH, \"20150602_083117\"))\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = MRDataset()\n",
    "dataset_val.load(os.path.join(DATA_PATH, \"20151026_082156\"))\n",
    "dataset_val.prepare()\n",
    "\n",
    "# Test dataset\n",
    "dataset_test = MRDataset()\n",
    "dataset_test.load(os.path.join(DATA_PATH, \"20151226_085413\"))\n",
    "dataset_test.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display random samples\n",
    "dataset = dataset_test\n",
    "image_ids = np.random.choice(dataset.image_ids, 4)\n",
    "\n",
    "# for ii in dataset.image_info:\n",
    "#     if ii['filename'] == '000005160.tif':\n",
    "#         image_ids = [ii['id']]\n",
    "#         break\n",
    "\n",
    "for image_id in image_ids:\n",
    "    image = dataset.load_image(image_id)\n",
    "    mask, class_ids = dataset.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset.class_names)\n",
    "    #print(dataset.image_info[image_id][\"filename\"])\n",
    "    #log(\"mask\", mask)\n",
    "    #log(\"class_ids\", class_ids)\n",
    "    #print(class_ids)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset = dataset_train\n",
    "avg0 = []\n",
    "avg1 = []\n",
    "avg2 = []\n",
    "for i in dataset.image_ids:\n",
    "    original_image, _, _, _, _ =\\\n",
    "        modellib.load_image_gt(dataset, config, \n",
    "                           i, use_mini_mask=False)\n",
    "    avg0.append(np.average(original_image[:,:,0]))\n",
    "    avg1.append(np.average(original_image[:,:,1]))\n",
    "    avg2.append(np.average(original_image[:,:,2]))\n",
    "        \n",
    "print(np.average(avg0), np.average(avg1), np.average(avg2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\",\n",
    "                          config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"imagenet\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\",\n",
    "                                \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\",\n",
    "                                \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model_path = model.find_last()\n",
    "    print(\"Loading weights from \", model_path)\n",
    "    model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train,\n",
    "            dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=90,\n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "\n",
    "#image augmentation: https://github.com/aleju/imgaug\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "seqAug = iaa.Sequential(\n",
    "    [\n",
    "        # apply the following augmenters to most images\n",
    "        iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
    "        iaa.Flipud(0.5), # vertically flip 20% of all images\n",
    "        # crop images by -10% to 10% of their height/width\n",
    "#         sometimes(iaa.CropAndPad(   # !!! Looks like memory curruption is hapenning somewhere in this C++ impl \n",
    "#             percent=(-0.1, 0.1),\n",
    "#             pad_mode=ia.ALL,\n",
    "#             pad_cval=0\n",
    "#         )),\n",
    "        sometimes(iaa.Affine(\n",
    "            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis\n",
    "            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # translate by -20 to +20 percent (per axis)\n",
    "            rotate=(-175, 175), # rotate by -175 to +175 degrees\n",
    "            shear=(-16, 16), # shear by -16 to +16 degrees\n",
    "            order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
    "            cval=0, # if mode is constant, use a cval = 0\n",
    "            mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "        ))\n",
    "    ],\n",
    "    random_order=True\n",
    ")\n",
    "\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 100.0,\n",
    "            epochs=180, \n",
    "            layers=\"all\",\n",
    "            augmentation=seqAug\n",
    "           )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(MRConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    DETECTION_MIN_CONFIDENCE = 0.9\n",
    "    DETECTION_NMS_THRESHOLD = 0.2\n",
    "    \n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "model_path = model.find_last()\n",
    "# model_path = os.path.join(ROOT_DIR, \"./logs/r5_imgaug_roi1000_20180608T1627/mask_rcnn_r5_imgaug_roi1000__0570.h5\")\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "dataset = dataset_test\n",
    "\n",
    "image_id = random.choice(dataset.image_ids)\n",
    "\n",
    "# for ii in dataset.image_info:\n",
    "#     if ii['filename'] == '000005160.tif':\n",
    "#         image_id = ii['id']\n",
    "#         break\n",
    "\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "# log(\"original_image\", original_image)\n",
    "# log(\"image_meta\", image_meta)\n",
    "# log(\"gt_class_id\", gt_class_id)\n",
    "# log(\"gt_bbox\", gt_bbox)\n",
    "# log(\"gt_mask\", gt_mask)\n",
    "# print(\"image_id: \", image_id)\n",
    "# print(dataset.image_info[image_id])\n",
    "\n",
    "results = model.detect([original_image], verbose=1)\n",
    "r = results[0]\n",
    "#if r[\"masks\"].shape[2] > 0:\n",
    "#    log(\"masks\", r[\"masks\"])\n",
    "\n",
    "ax, fig = get_ax(1,2)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset.class_names, ax=ax[0])\n",
    "\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset.class_names, r['scores'], ax=ax[1])\n",
    "\n",
    "AP, precisions, recalls, overlaps =\\\n",
    "    utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                     r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "print(AP)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"tank1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "dataset = dataset_val\n",
    "image_ids = dataset.image_ids\n",
    "APs = []\n",
    "pss = []\n",
    "rcs = []\n",
    "ops = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    pss.append(precisions)\n",
    "    rcs.append(recalls)\n",
    "    ops.append(overlaps)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
